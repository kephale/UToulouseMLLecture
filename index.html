<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    
    <title>Introduction to Machine Learning and Data Mining</title>
    
    <meta name="description" content="Introduction to Machine Learning and Data Mining">
    <meta name="author" content="Kyle I S Harrington">
    
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
    
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black_KISHtufts135_2016.css" id="theme">
    
    <!-- Code syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    
    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!-- Footer header 
    <link rel="stylesheet" href="plugin/title-footer/title-footer.css"> -->
    
    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>
  
  <body>
    <div class="reveal">
      <div class="footer">
	Universit√© Toulouse 1 Capitole - <a href="http://kyleharrington.com/">Kyle Harrington</a> - September 29, 2017
      </div>
      
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
	<section>
	  <h2>Introduction to Machine Learning and Data Mining</h2>
	  <p>
	    <small> <a href="http://kyleharrington.com">Kyle I S Harrington</a> / <a href="mailto:kharrington@uidaho.edu">kharrington@uidaho.edu</a></small>
	  </p>
	  <br><br><br><br><br><br>
	  <p><small>Some slides adapted from K. Harrington's CS-135 Machine Learning course at Tufts University.</small></p>
	</section>

	<section>
    <section data-markdown>
      ## How I got here (up to PhD)

      - Bachelor's Degree in Artificial Life, 2007
      - Instructor of Computer Science (Hampshire College), 2008
      - Masters in Computer Science (Brandeis University), 2010
      - Visiting Scholar, University of Massachusetts, Amherst, 2010 to 2012
      - PhD in Computer Science with specialization in Quantitative Biology (Brandeis University), 2014

    </section>
    <section data-markdown>
      ## How I got here (PhD to now)
      
      - Visiting Scientist at Howard Hughes Medical Institute, 2014
      - Postdoc at Harvard Medical School, 2014 to 2016
      - Assistant Professor at University of Idaho, 2016 to **now**
      - Visiting Faculty at Max Planck Institute, 2017
      - Visiting Faculty at UT1, 2017

    </section>
    <section data-markdown>
      ## Current Research Group:

      - Kyle Harrington (Group Leader)
      - Leo Epstein (Graduate student)
      - Zeth duBois (Graduate student)
      - Louise Magbunduku (Undergraduate student)

    </section>    
    </section>

	
	<section>
	<section>
	  <h2>Who Likes Tennis?</h2>
	  <table>
	    <tr><td>
		<table style="font-size:18px">
		  <tr><td>Outlook</td><td>Temp</td><td>Humidity</td><td>Windy</td><td>Play</td></tr>
		  <tr><td>Sunny</td><td>Hot</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>Sunny</td><td>Hot</td><td>High</td><td>True</td><td>No</td></tr>
		  <tr><td>Overcast</td><td>Hot</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Rainy</td><td>Mild</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Rainy</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Rainy</td><td>Cool</td><td>Normal</td><td>True</td><td>No</td></tr>
		  <tr><td>Overcast</td><td>Cool</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>Sunny</td><td>Mild</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>Sunny</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Rainy</td><td>Mild</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Sunny</td><td>Mild</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>Overcast</td><td>Mild</td><td>High</td><td>True</td><td>Yes</td></tr>
		  <tr><td>Overcast</td><td>Hot</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>Rainy</td><td>Mild</td><td>High</td><td>True</td><td>No</td></tr>
	  </table>
	      </td><td>
		<br>
		<p>Class: Play tennis?</p>
		<p>Attributes:</p>
		<ul>
		  <li>Outlook $\in$ [Sunny, Overcast, Rainy]</li>
		  <li>Temp $\in$ [Hot, Mild, Cool]</li>
		  <li>Humidity $\in$ [High, Normal]</li>
		  <li>Windy $\in$ [True, False]</li>
	  </td></tr></table>
	</section>

	<section>
	  <h2>Who Likes Tennis?</h2>
	  <img src="images/davidLin_Hedberg_tennis.png" width=70%>
	</section>
	
	<section>
	  <h2>Decision Trees</h2>
	  <img src="images/Mitchell_playTennis_decision_tree.png" width=90%>
	</section>

 	<section>
	  <h2>Representation of a Decision Tree</h2>
	  <p>Nodes are attributes</p>
	  <p>Branches are values of attributes</p>
	  <p>Leafs are classes</p>
	  <img src="images/Mitchell_playTennis_decision_tree.png" width=40%>
	</section>	

<!-- 	<section>
	  <h2>When to Use a Decision Tree?</h2>
	  <p>Discrete attribute-values (although continuous-value implementations work as well)</p>
	  <p>Discrete outputs</p>
	  <p>It is OK to have noise in training data</p>
	  <p>It is OK to have missing values</p>	  	  
	</section>
-->

	<section>
	  <h2>Decision Trees</h2>
	  <ol>
	    <li>Choose the best attribute, $A$, for the next node</li>
	    <li>Create a node for $A$</li>
	    <li>Create branches for each possible value of $A$</li>
	    <li>Sort all observations into leaves</li>
	    <li>If all observations are perfectly classified,
	      <br>then stop,
	      <br>else recur.</li>
	  </ol>
	  <p>How do we choose the best attribute?</p>
	</section>

 	<section>
	  <h2>Choosing an Attribute</h2>
	  <img src="images/Mitchell_choose_a_node.png">
	  <p>Which is better A1 or A2?</p>
	</section>
	</section>	

	<section>
	<section>
	  <h2>Entropy</h2>
	  <p>Entropy describes how unpredictable information is.</p>
	  <table>
	    <tr><td><img src="images/Mitchell_entropy.png" width="400"></td>
	      <td>
		<ul>
		  <li>$S$ is the sample of training data</li>
		  <li>$p_\oplus$ is the fraction of positive samples</li>
		  <li>$p_\ominus$ is the fraction of negative samples</li>
		  <li>$Entropy(S) \equiv -p_\oplus log_2 p_\oplus - p_\ominus log_2 p_\ominus$</li>
		</ul>
	      </td>
	    </tr>
	  </table>
	</section>


	<section>
	  <h2>Entropy</h2>
	  <p>$Entropy(S)$ is the expected number of bits needed to encode a class of a randomly drawn member of the sample set (under the optimal, shortest-length code)</p>
	</section>

	<section>
	  <h2>Entropy</h2>
	  <p>An optimal length code uses $-log_2 p$ bits to encode a message of probability p</p>
	  <p>i.e. if $p=0.01$, then the optimal code length is $6.64$</p>
	  <p>and if $p=0.99$, then the optimal code length is $0.01$</p>
	</section>

	<section>
	  <h2>Entropy</h2>
	  <p>Recall that our sample set $S$ has $p_\oplus$ and $p_\ominus$ proportions of $\oplus$ and $\ominus$ classes</p>
	  <p>Hence, to encode a random member of $S$, weigh the probability of drawing a member of the class by the optimal length code of the class:</p>
	  <p>$p_\oplus (-log_2 p_\oplus) + p_\ominus ( -log_2 p_\ominus )$</p>
	  <p>$Entropy(S) \equiv -p_\oplus log_2 p_\oplus - p_\ominus log_2 p_\ominus$</p>	  
	  <!--- <small>Go read Claude Shannon's "A Mathematical Theory of Communication" (1948) if this excites you.</small> -->
	</section>
	</section>

 	<section>	
 	<section>
	  <h2>Choosing an Attribute</h2>
	  <img src="images/Mitchell_choose_a_node.png">
	  <p>How can we use entropy in this decision?</p>
	</section>

	<section>
	  <h2>Information Gain</h2>
	  <p>$Gain(S,A)$ is the expected reduction in entropy by sorting on attribute $A$</p>
	  <p>$Gain(S,A) \equiv Entropy(S) - \displaystyle \sum_{v \in Values(A)} \frac{|S_v|}{|S|} Entropy(S_v)$</p>
	  <img src="images/Mitchell_choose_a_node.png">
	</section>

	<section>
	  <h2>Information Gain Example</h2>
	  <img src="images/Mitchell_humidity_or_wind.png">
	</section>

	<section>
	  <h2>Building a Tree Example</h2>
	  <table>
	    <tr><td>
		<table style="font-size:18px">
		  <tr><td>Day</td><td>Outlook</td><td>Temp</td><td>Humidity</td><td>Windy</td><td>Play</td></tr>
		  <tr><td>D1</td><td>Sunny</td><td>Hot</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>D2</td><td>Sunny</td><td>Hot</td><td>High</td><td>True</td><td>No</td></tr>
		  <tr><td>D3</td><td>Overcast</td><td>Hot</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D4</td><td>Rainy</td><td>Mild</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D5</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D6</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>True</td><td>No</td></tr>
		  <tr><td>D7</td><td>Overcast</td><td>Cool</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D8</td><td>Sunny</td><td>Mild</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>D9</td><td>Sunny</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D10</td><td>Rainy</td><td>Mild</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D11</td><td>Sunny</td><td>Mild</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D12</td><td>Overcast</td><td>Mild</td><td>High</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D13</td><td>Overcast</td><td>Hot</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D14</td><td>Rainy</td><td>Mild</td><td>High</td><td>True</td><td>No</td></tr>
		</table>
	      </td><td>
		<br>
		<img src="images/Mitchell_choose_an_attribute.png">
	    </td></tr>	    
	  </table>
        </section>

		<section>
	  <h2>Building a Tree Example</h2>
	  <table>
	    <tr><td>
		<table style="font-size:18px">
		  <tr><td>Day</td><td>Outlook</td><td>Temp</td><td>Humidity</td><td>Windy</td><td>Play</td></tr>
		  <tr><td>D1</td><td>Sunny</td><td>Hot</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>D2</td><td>Sunny</td><td>Hot</td><td>High</td><td>True</td><td>No</td></tr>
		  <tr><td>D3</td><td>Overcast</td><td>Hot</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D4</td><td>Rainy</td><td>Mild</td><td>High</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D5</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D6</td><td>Rainy</td><td>Cool</td><td>Normal</td><td>True</td><td>No</td></tr>
		  <tr><td>D7</td><td>Overcast</td><td>Cool</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D8</td><td>Sunny</td><td>Mild</td><td>High</td><td>False</td><td>No</td></tr>
		  <tr><td>D9</td><td>Sunny</td><td>Cool</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D10</td><td>Rainy</td><td>Mild</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D11</td><td>Sunny</td><td>Mild</td><td>Normal</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D12</td><td>Overcast</td><td>Mild</td><td>High</td><td>True</td><td>Yes</td></tr>
		  <tr><td>D13</td><td>Overcast</td><td>Hot</td><td>Normal</td><td>False</td><td>Yes</td></tr>
		  <tr><td>D14</td><td>Rainy</td><td>Mild</td><td>High</td><td>True</td><td>No</td></tr>
		</table>
	      </td><td>
		<br>
		<img src="images/Mitchell_choose_an_attribute_math.png">
	    </td></tr>	    
	  </table>
		</section>

	<section>
	  <h2>Searching Hypothesis Space with ID3</h2>
	  <p>ID3 Algorithm: grow decision tree using information gain</p>
	  <img src="images/Mitchell_ID3_hypothesis_space_search.png" width=40%>
	</section>
		
 	<section>
	  <h2>Properties of ID3</h2>
	  <p>For any given training set</p>
	  <p>Is it always possible to build a tree?</p>
	  <p>If so, will it be a good tree?</p>	  
	</section>

 	<section>
	  <h2>Properties of ID3</h2>
	  <p>Only 1 hypothesis (decision tree)</p>
	  <p>If an early split goes wrong, we're stuck with it!</p>
	  <p>Uses class statistics, robust to noise</p>
	  <p>Inductive bias of ID3: tends to prefer shorter trees</p>
	</section>
		
 	<section>
	  <h2>Inductive Bias of ID3</h2>
	  <p>Favors shorter trees/more information gain closer to the root</p>
	  <p>The bias arises from the search, not the search space</p>
	  <p>Occam's razor: the simplest hypothesis that fits is the best one</p>
	</section>
	</section>

	<section>
	<section>
	  <h2>Overfitting</h2>
	  <p>Let's describe the error of a hypothesis $h$ as $error(h)$</p>
	  <p>Now consider specific error measurements:</p>
	  <ul>
	    <li>training set, $S$: $error_S(h)$</li>
	    <li>full dataset, $D$: $error_D(h)$</li>
	  </ul>
	</section>

	<section>
	  <h2>Overfitting</h2>
	  <br>
	  <p>Hypothesis $h$ overfits if:</p>
	  <p>$error_S(h) < error_S(h')$</p>
			   <p>and</p>
	  <p>$error_D(h) > error_D(h')$</p>
	</section>

	<section>
	  <h2>Overfitting</h2>
	  <img src="images/Mitchell_overfitting.png" width=90%>
	</section>

	<section>
	  <h2>Causes of overfitting</h2>
	  <p>Not enough examples on some attributes</p>
	  <p>Noisy data</p>
	</section>

	<section>
	  <h2>Preventing Overfitting</h2>
	  <p>Stop growing when a new split isn't statistically significant</p>
	  <p>Grow and prune post-hoc</p>
	</section>

	<section>
	  <h2>Reduced-error pruning</h2>
	  <ul>
	    <li>Build a tree as usual, potentially overfitting</li>
	    <li>Use a validation dataset</li>
	    <li>Greedily remove nodes that improve the accuracy on the validation data</li>
	  </ul>
	  <p>Limitations?</p>
	</section>

	<section>
	  <h2>Reduced-error pruning</h2>
	  <img src="images/Mitchell_overfitting_pruning.png" width=90%>
	</section>
	</section>

 	<section>	  	
 	<section>	  
	  <h2>Features</h2>
	  <p>The input to a ML algorithm/model is composed features (aka attributes)</p>
	  <p>The output is a class or a value</p>
	  <!-- <img src="images/ML_blackbox.svg" />	  -->
	</section>

	<section>
	  <h2>The Black Box Delusion</h2>
	  <p>Can we just take tons of measurements, feed them into our ML algorithm, and start making predictions?</p>
	</section>

	<section>
	  <h2>Features</h2>
	  <ul>
	    <li>More features generally slow ML algorithms down</li>
	    <li>Irrelevant features can inhibit performance</li>
	  </ul>
	  <p>What can we do about this?</o>
	</section>
	  
	<section>
	  <h2>Feature Selection</h2>
	  <ul>
	    <li>Eliminate features</li>
	    <li>Choose subsets of features that work well</li>
	    <li>Build it into the ML algorithm</li>
	  </ul>
	</section>
	</section>
	
	<section>
	<section>
	  <h2>Instance Transformation</h2>
	  <p>Our dataset $D$ has $N$ dimensions, for $N$ features</p>
	  <p>Instance transformations reduce the number of dimensions by transforming the features themselves</p>
	</section>

	<section>
	  <h2>Instance Transformation</h2>
	  <p>If we had a dataset with red, green, blue, and yellow features (N=4),</p>
	  <p>Then we might transform the dataset to $( red - green ) / (red + green)$ and $ (blue-yellow) / (blue + yellow)$</p>
	</section>	
	
	<section>
	  <h2>Principle Component Analysis</h2>
	  <p>Principle component analysis (PCA) maps from one set of axes to orthogonal axes</p>
	  <p>Roughly speaking, project onto the axes of highest variation</p>
	</section>

	<section>
	  <h2>Principle Component Analysis</h2>
	  <p>Eigenface reduces faces to low-dimensional space</p>
	  <table>
	    <tr>
	      <td>Bases</td>
	      <td>Generated faces</td>
	    </tr>
	    <tr>
	      <td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Eigenfaces.png/220px-Eigenfaces.png"></td>
	      <td><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/FaceMachine_screenshots_collage.jpg/1024px-FaceMachine_screenshots_collage.jpg" width=50%></td>
	    </tr>
	  </table>
	</section>	

	<!--
	<section>
	  <h2>Manifold Methods</h2>
	  <p>Datasets can be thought of as manifolds</p>
	  <img src="images/manifold_methods.png"><small>(Saul and Roweis, 2003)</small><br>
	  <p>Embed data in a low dimensional space, while preserving distance between points</p>
        </section>

	
	<section>
	  <h2>Instance Transformation</h2>
	  <p>Instance transformations reduce the number of dimensions by transforming the features themselves</p>
	  <p>Most methods of instance transformation are unsupervised</p>
	</section>	
-->	
	</section>

	<section>
	<section>
	  <h2>Filter Methods</h2>
	  <p>Filter irrelevant features based upon the dataset</p>
	</section>

	<section>
	  <h2>Filter Methods</h2>
	  <ul>
	    <li>Assign a score to each feature with a heuristic</li>
	    <li>Filter out useless features</li>
	  </ul>
	</section>	
	  
	<section>
	  <h2>Filter Methods</h2>
	  <p>Ranking features based upon correlation between feature and class</p>
	  <p>$Rank(f) = \frac{ E[( X_f - \mu_{X_f} ) ( Y - \mu_Y ) ] }{ \sigma_{X_f} \sigma_Y }$</p>
	  <p>where $f$ is the feature of interest, and $Y$ is the class</p>
	</section>

	<section>
	  <h2>Filter Methods</h2>
	  <p>Mutual information between a feature and class</p>
	  <p>$Rank(f) = \displaystyle \sum_{X_f} \displaystyle \sum_Y p(X_f,Y) log \frac{ p(X_f,Y) }{ p(X_f) p(Y) }$</p>
	  <p>where $f$ is the feature of interest, and $Y$ is the class</p>	  
	</section>

	<section>
	  <h2>Filter Methods</h2>
	  <ul>
	    <li>Assign a score to each feature with a heuristic</li>
	    <li>Filter out useless features</li>
	  </ul>
	  <p>What issues could there be?</p>
	  <p>How necessary is this?</p>
	</section>
	
	<section>
	  <h2>Filter Methods</h2>
	  <p>Issues:</p>
	  <ul>
	    <li>How many features do we keep?</li>
	    <li>Heuristics are only applied to 1 feature</li>
	  </ul>
	</section>			
	</section>

	<section>
	<section>
	  <h2>Wrapper Method</h2>
	  <ol>
	    <li>Select a subset of features</li>
	    <li>Run a ML algorithm</li>
	    <li>Use performance of ML algorithm to choose best subset</li>
	  </ol>
	  <p>What is appealing about this?</p>
	</section>

	<section>
	  <h2>Wrapper Method</h2>
	  <p>Advantages of using validation sets</p>
	  <p>Features are tailored to ML algorithm</p>
	  <p>Considers different ways of combining features</p>
	</section>	
	  
	<section>
	  <h2>Wrapper Method: Forward Selection</h2>
	  <p>Start with subsets of only 1 feature</p>
	  <p>Grow subset of features by adding 1 new feature per iteration</p>
	</section>

	<section>
	  <h2>Wrapper Method: Backward Elimination</h2>
	  <p>Start with the full set of features</p>
	  <p>Eliminate 1 feature per iteration</p>
	</section>

	<section>
	  <h2>Wrapper Method: Alternatives</h2>
	  <p>Exhaustive search (consider all subsets)</p>
	  <p>Alternative AI methods (simulated annealing, genetic algorithms, ...)</p>
	  <p>Feature subset search is NP-hard</p>
	</section>		
	</section>

	<section>
	  <h2>Comparing Filter and Wrapper Methods</h2>
	  <p>Filtering: 1-step process, considers features independently</p>
	  <p>Wrapper: iterates through subsets of features, selects subset that matches ML algorithm</p>
	</section>
	
<!--
	<section>
	  <h2>Feature Selection within ML Algorithm</h2>
	  <p>When searching/optimizing, provide an incentive to be simple (sparse) by penalizing complexity</p>	  
	  <p>Will be covered later (L1 regularization)</p>
	</section>
	<section>
	  <h2>Feature Preprocessing</h2>
	  <p>The range of values for a given feature can impact an algorithm's performance</p>
	  <p>Remember using the value of the year directly on assignment 1?</p>
	</section>
-->
	
	<section>

	<section>
	  <h2>Linear scaling</h2>
	  <p>Scale the values into the range $[0,1]$</p>
	  <p>$x \leftarrow \frac{ x - x_{min} }{ x_{max} - x_{min} }$</p>
	  <p>Scale based on training set only</p>
	</section>

	<section>
	  <h2>Z-normalization</h2>
	  <p>Scale the distribution to have mean=0 and std=1</p>
	  <p>$x \leftarrow \frac{ x - \mu_X }{ \sigma_X }$</p>
	  <p>Scale based on training set only</p>
	</section>			  

	<section>
	  <h2>Feature Discretization</h2>
	  <p>Some algorithms only work on discrete features</p>
	  <p>We may need to discretize real-valued features</p>	  
	</section>

	<section>
	  <h2>Feature Discretization</h2>
	  <p>Calculate the histogram</p>
	  <p>This divides the values into bins</p>
	  <ul>
	    <li>Equal bin sizes</li>
	    <li>Equal # of instances per bin</li>
	  </ul>
	</section>

	<section>
	  <h2>Feature Discretization</h2>
	  <p>Alternatively, use a heuristic/ad-hoc method to discretize in a useful way</p>
	  <p>E.G. Build a decision tree, let the DT algorithm discretize, and use the split values of the optimized tree</p>
	</section>

	<section>
	  <h2>From Discrete to Numerical</h2>
	  <p>Some features are unordered (i.e. Browsers = [ Firefox, Chrome, Safari ])</p>
	  <p>Most common approach is to use unit vectors:</p>
	  <table>
	    <tr><td>Firefox</td><td>Chrome</td><td>Safari</td></tr>
	    <tr><td>1</td><td>0</td><td>0</td></tr>
	    <tr><td>0</td><td>1</td><td>0</td></tr>
	    <tr><td>0</td><td>0</td><td>1</td></tr>
	  </table>
	</section>
	</section>	

	<!--
	<section>
 	<section>
	  <h2>Model Selection</h2>
	  <p>Multiple models can often describe the same data,</p>
	  <p>How do we choose which one to use?</p>
	</section>

 	<section>
	  <h2>Model Selection</h2>
	  <p>We may favor the model with lower error/higher accuracy</p>
	  <p>Even so, is the model with the lowest error on a particular set of data the one that we want?</p>
	</section>
	  
	<section>
	  <h2>True error</h2>
	  <img src="images/snowdaytree_depth1.png">
	  <p>$error_D(h) \equiv Pr_{x \in D} [f(x) \neq h(x)]$</p>
	  <ul>
	    <li>f is our target function</li>
	    <li>D is the complete distribution/dataset</li>
	  </ul>
	</section>
	  
	<section>
	  <h2>Sample error</h2>
	  <img src="images/snowdaytree_depth1.png">
	  <p>$error_S(h) \equiv \frac{1}{n} \displaystyle \sum_{x \in S} \delta(f(x) \neq h(x))$</p>
	  <ul>
	    <li>S is the sample dataset</li>
	    <li>$\delta(f(x) \neq h(x)) = 1$ if the condition is true, otherwise $0$</li>
	  </ul>	  
	</section>

 	<section>
	  <h2>True error v. Sample error</h2>
	  <p>How well does $error_S(h)$ estimate $error_D(h)$?</p>
	</section>

 	<section>
	  <h2>True error v. Sample error</h2>
	  <p>Consider the case where $S$ is our training set:</p>
	  <p>$bias \equiv E[error_S(h)] - error_D(h)$</p>
	  <p>For any training set, we expect that bias will be negative because $h$ and $S$ are not independent</p>
	  <p>What other complications beyond $bias$ might arise?</p>
	</section>

 	<section>
	  <h2>True error v. Sample error</h2>
	  <p>Even if $S$ and $h$ are independent, $error_S(h)$ may vary from $error_D(h)$</p>
	</section>

 	<section>
	  <h2>Sample error</h2>
	  <p>$error_S(h)$ is a random variable</p>
	  <p>If we rerun with a different randomly drawn $S$, where $|S| = n$</p>
	  <img src="images/mitchell_binomial_n40_p0.3.png" width=50%>	  
	  <p>$P( n * error_S(h) ) = P(r) = \frac{ n! } { r! ( n - r )! } p^r ( 1 - p )^{n-r}$,</p>
	  <p> where $p = error_D(h)$</p>
	</section>			
	</section>
	
	<section>
 	<section>
	  <h2>Samples and Stats</h2>
	  <p>A random variable, $Y$, has a value representing the outcome of an experiment (a coin lands heads up)</p>
	  <p>Probability distribution for a random variable is the probability $Pr(Y=y_i)$ that Y will be $y_i$ for each possible $y_i$</p>
	</section>	

 	<section>
	  <h2>Samples and Stats</h2>
	  <p>Expected value (aka mean) : $\mu_Y = E[Y] = \displaystyle \sum_i y_i Pr(Y=y_i)$</p>
	  <p>Variance : $Var(Y) = E[(Y-\mu_Y)^2]$ - How far the distribution spreads about the mean</p>
	  <p>Standard deviation : $\sigma_Y = \sqrt{ Var(Y) }$</p>	  
	</section>
	</section>

	<!--
 	<section>
 	<section>
	  <h2>Normal Distribution</h2>
	  <p>Bell curve described by a mean and standard deviation</p>
	  <img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG" width=60%>
	</section>

 	<section>
	  <h2>Normal Distributions</h2>
	  <p>90% of area (probability) lies in $\mu \pm 1.64 \sigma$</p>
	  <p>N% of area (probability) lies in $\mu \pm z_N \sigma$</p> 
	  <table style="font-size:28px">
	    <tr><td>N%</td><td>50%</td><td>68%</td><td>80%</td><td>90%</td><td>95%</td><td>98%</td><td>99%</td></tr>
	    <tr><td>$z_N$</td><td>0.67</td><td>1.00</td><td>1.28</td><td>1.64</td><td>1.96</td><td>2.33</td><td>2.58</td></tr>
	  </table>
	</section>

 	<section>
	  <h2>Normal Distributions to Probabilities</h2>
	  <p>$x \in \mu \pm z_N \sigma$, translates to: "with probability N%"</p>
	  <p>$\mu \in x \pm z_N \sigma$, translates to: "with confidence N%"</p>
	</section>

 	<section>
	  <h2>Normal distributions to Probabilities</h2>
	  <p>Consider $x$, drawn from $\mathcal{N}(\mu,\sigma)$</p>
	  <p>We say: with confidence 95%, $\mu \in x \pm 1.96 \sigma$</p>
	</section>

 	<section>
	  <h2>Normal distributions to Probabilities</h2>
	  <p>If sample data $S$ contains $n \geq 30$ samples drawn independent of $h$ and each other,</p>
	  <p>Then with approx. 95% probability $error_S(h)$ lies in interval</p>
	  <p>$error_D(h) \pm 1.96 \sqrt{\frac{error_D(h) (1 - error_D(h))}{n}}$</p>
	</section>	
	</section>

	<section>
	<section>
	  <h2>Central Limit Theorem</h2>
	  <p>Consider a set of independent, identically distributed random variables $Y_1 ... Y_n$, drawn from an arbitrary probability distribution with mean $\mu$ and finite variance $\sigma^2$. The sample mean is</p>
	  <p>$\bar{Y} \equiv \frac{1}{n} \displaystyle \sum^{n}_{i=1} Y_i$</p>
	</section>

	<section>
	  <h2>Central Limit Theorem</h2>
	  <p>$\bar{Y} \equiv \frac{1}{n} \displaystyle \sum^{n}_{i=1} Y_i$</p>
	  <p>As $n \rightarrow \infty$, the distribution governing $\bar{Y}$ approaches a Normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$</p>
	</section>

	<section>
	  <h2>Central Limit Theorem</h2>
	  <img src="images/weibull-1.JPG" width=35%>
	  <p>Yellow indicates the shape of underlying distribution (Weibull), blue bars are 50 averages of 1 sample</p>
	</section>

	<section>
	  <h2>Central Limit Theorem</h2>
	  <img src="images/weibull-5.JPG" width=35%>
	  <p>Yellow indicates the shape of underlying distribution, blue bars are 50 averages of 5 samples</p>
	</section>

	<section>
	  <h2>Central Limit Theorem</h2>
	  <img src="images/weibull-25.JPG" width=35%>
	  <p>Yellow indicates the shape of underlying distribution, blue bars are 50 averages of 25 samples</p>
	  <p><small><a href="http://blog.gembaacademy.com/2007/07/16/explaining-the-central-limit-theorem/">Detailed explanation</a></small></p>
	</section>		

	<section>
	  <h2>From Binomial to Normal</h2>
	  <p>By applying the central limit theorem we can approximate a binomial distribution with a normal distribution</p>
	  <p>We can then say:</p>
	  <ul>
	    <li>$\mu_{error_S(h)} = error_S(h)$</li>
	    <li>$\sigma_{error_S(h)} \approx \sqrt{ \frac{error_S(h) (1 - error_S(h)) } {n} }$</li>
	  </ul>
	</section>	
	</section>
	-->
<!--
	<section>	  
 	<section>
	  <h2>Measuring Classifier Performance</h2>
	  <p>If we use a classifier on a test set with $n$ samples</p>
	  <p>We estimate the error rate as $\hat{p} = \frac{|incorrect|}{n}$</p>
	  <p>If $n \geq 30$, (use C.L.T.) $\hat{p}$ is approx. distributed as $\hat{p} = \mathcal{N}(\mu, \sigma)$</p>
	</section>
	
 	<section>
	  <h2>Measuring Classifier Performance</h2>
	  <p>If $n \geq 30$, (use C.L.T.) $\hat{p}$ is approx. distributed as $\hat{p} = \mathcal{N}(\mu, \sigma)$</p>
	  <p>$\mu$ is $error_D(h)$, and $\sigma$ is $\sqrt{ \frac{error_D(h) (1-error_D(h)) }{n} }$</p>
	  <p>Oi, we're stuck with $error_D(h)$'s in our expression!</p>
	</section>

	</section>

	<section>
	  <section>
	  <h2>Comparing Algorithms</h2>
	  <p>Consider 2 hypotheses $h_1$ and $h_2$, each tested on an independenly generated sample set from the same distribution</p>
	  <p>It would be interesting to know the difference in error</p>
	  <p>$d \equiv error_D(h_1) - error_D(h_2)$</p>
	  </section>

	  <section>
	  <h2>Comparing Algorithms</h2>
	  <p>Target: $d = error_D(h_1) - error_D(h_2)$</p>
	  <p>Estimate with: $\hat{d} \equiv error_{S_1}(h_1) - error_{S_2}(h_2)$</p>	  
	  </section>

	  <section>
	    <h2>Comparing Algorithms</h2>
	    <p>Estimate with: $\hat{d} \equiv error_{S_1}(h_1) - error_{S_2}(h_2)$</p>	  	    
	    <p>Use C.L.T., to estimate the distribution</p>
	    <p>$\sigma_{\hat{d}} = \sqrt{ \frac{error_{S_1}(h_1) (1 - error_{S_1}(h_1))}{ n_1} + \frac{ error_{S_2}(h_2) ( 1 - error_{S_2}(h_2) ) }{n_2} }$</p>
	  </section>

	  <section>
	    <h2>Comparing Algorithms</h2>
	    <p>Find the lower and upper limit of the interval such that N% of probability mass is within the interval:</p>
	    <p>$\hat{d} \pm z_N \sqrt{ \frac{error_{S_1}(h_1) (1 - error_{S_1}(h_1))}{ n_1} + \frac{ error_{S_2}(h_2) ( 1 - error_{S_2}(h_2) ) }{n_2} }$</p>
	    <p>Lookup in the table of $N%$ to $z_N$ values</p>
	  </section>

	  <section>
	    <h2>A Method for Comparing Algorithms</h2>
	    <p>Now consider hypotheses $h_A$ and $h_B$</p>
	    <p>Partition into k-folds, $T_1 ... T_k$, of equal size $\geq 30$</p>
	    <p>For $i$ from $1$ to $k$: $\delta_i \leftarrow error_{T_i}(h_A) - error_{T_i}(h_B)$</p>
	  </section>

	  <section>
	    <h2>A Method for Comparing Algorithms</h2>
	    <p>For $i$ from $1$ to $k$: $\delta_i \leftarrow error_{T_i}(h_A) - error_{T_i}(h_B)$</p>
	    <p>N% confidence interval estimate:</p>
	    <p>$\bar{ \delta } \pm t_{N,k-1} s_{\bar{\delta}}$</p>
	    <p>$s_{\bar{\delta}} \equiv \sqrt{ \frac{1}{k(k-1)} \displaystyle \sum_{i=1}^k (\delta_i - \bar{ \delta })^2 }$</p>
	    <p>Where did $t_{N,k-1}$ come from?</p>
	  </section>

	  <section>
	    <h2>A Method for Comparing Algorithms</h2>
	    <img src="images/t_table.png" width=55%><br>
	    <small><a href="http://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf">Table source</a></small>
	  </section>

	  <section>
	    <h2>A Method for Comparing Algorithms</h2>
	    <p>Now consider hypotheses $h_A$ and $h_B$</p>
	    <p>Target: $d = error_D(h_A) - error_D(h_B)$</p>
	    <p>N% confidence interval estimate:</p>
	    <p>$\bar{ \delta } \pm t_{N,k-1} s_{\bar{\delta}}$</p>
	    <p>$s_{\bar{\delta}} \equiv \sqrt{ \frac{1}{k(k-1)} \displaystyle \sum_{i=1}^k (\delta_i - \bar{ \delta })^2 }$</p>
	  </section>	  
	</section>

	<!--
	<section>
	  <section>
	    <h2>Model Selection</h2>
	    <p>What did we just do?</p>
	  </section>

	  <section>
	    <h2>Model Selection</h2>
	    <ul>
	      <li>Estimating true error from a sample set</li>
	      <li>Sample error follows a binomial distribution</li>
	      <li>Using the Central Limit Theorem, sampling can lead to a normal distribution</li>
	      <li>Hypotheses can be compared with a t-test</li>
	    </ul>
	  </section>	  
	</section>

	<section>	
	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <p>If we believe something about our data,</p>
	  <p>how do we express the accuracy of this belief?</p>	  
	</section>

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<img src="images/snowdaytree_depth1.png">
	      </td>
	    </tr>
	  </table>
	  <p>How accurate is this decision tree?</p>
	</section>

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<img src="images/snowdaytree_depth1.png">
	      </td>
	    </tr>
	  </table>
	  <p>Correct: 11, Incorrect: 1</p>
	</section>	


	<section>
	  <h2>Confusion Matrix</h2>
	  <table>
	    <tr><td></td><td>+</td><td>-</td><td>Prediction</td></tr>
	    <tr><td>+</td><td>TP</td><td>FN</td><td></td></tr>
	    <tr><td>-</td><td>FP</td><td>TN</td><td></td></tr>
	    <tr><td>Reality</td><td></td><td></td><td></td></tr>	      	    
	  </table>
	</section>

	<section>
	  <h2>Accuracy</h2>
	  <table>
	    <tr><td></td><td>+</td><td>-</td><td>Prediction</td></tr>
	    <tr><td>+</td><td>TP</td><td>FN</td><td></td></tr>
	    <tr><td>-</td><td>FP</td><td>TN</td><td></td></tr>
	    <tr><td>Reality</td><td></td><td></td><td></td></tr>	      	    
	  </table>
	  <p>$Acc = \frac{TP+TN}{TP+TN+FP+FN}$</p>
	</section>

	<section>
	  <h2>Accuracy</h2>
	  <p>$Acc = \frac{TP+TN}{TP+TN+FP+FN}$</p>
	  <p>What if we calculated a snowday predictor over the entire academic year?</p>
	  <p>Should we optimize with respect to $Acc$?</p>
	</section>

	<section>
	  <h2>Precision and Recall</h2>
	  <p>In information retrieval, we search to satisfy some $Query$</p>
	  <p>Goal is to optimize positive query responses</p>
	  <p>$Precision = \frac{TP}{TP+FP}$</p>
	  <p>$Recall = \frac{TP}{TP+FN}$</p>
	  <p>This is often combined into the F-score</p>
	  <p>$F = \frac{2 * P * R}{P + R}$</p>
	</section>

	<section>
	  <h2>Sensitivity and Specificity</h2>
	  <p>In medicine, we want accurate diagnoses</p>
	  <p>$Sensitivity = \frac{TP}{TP+FN}$ (same as recall)</p>	  
	  <p>$Specificity = \frac{TN}{TN+FP}$</p>
	  <p>How "accurate" the trues and falses are</p>
	</section>

	<section>
	  <h2>True and False Positives</h2>
	  <p>In signal detection, we want to find a signal within data</p>
	  <p>$TPrate = \frac{TP}{TP+FN} = recall$</p>	  
	  <p>$FPrate = \frac{FP}{TN+FP} = 1 - Specificity$</p>
	  <p>Often represented graphically as ROC (receiver operator characteristic) curve plots</p>
	</section>

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<p>Let's consider a linear regression:</p>
		<p style="font-size:24px">$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
		<br>
		<p>We need everything to be a number, so lets find-and-replace:</p>
		<p>Light = 0, Medium = 1, Heavy = 2</p>
		<p>TRUE = 1, FALSE = 0</p>
	      </td>
	    </tr>
	  </table>
	</section>		  

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<p>Let's consider a linear regression:</p>
		<p style="font-size:24px">$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
		<p style="font-size:24px">Previous morning, $w_1 = -0.18649558$</p>
		<p style="font-size:24px">Previous day, $w_2 = 0.07225614$</p>
		<p style="font-size:24px">Previous night, $w_3 = 0.15659649$</p>
		<p style="font-size:24px">Early morning, $w_4 = 0.39815622$</p>
	      </td>
	    </tr>
	  </table>
	</section>

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<p>Let's consider a linear regression:</p>
		<p style="font-size:24px">$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
		<p style="font-size:24px">$Closed = w_1 * Light + w_2 * Light$</p>
		<p style="font-size:24px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$+ w_3 * Light + w_4 * Heavy$</p>
		<p style="font-size:24px">$1.01 = -0.19 * 0 + 0.07 * 0$</p>
		<p style="font-size:24px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$+ 0.16 * 0 + 0.40 * 2$</p>
	      </td>
	    </tr>
	  </table>
	</section>

	<section>
	  <h2>How accurate is a hypothesis?</h2>
	  <table>
	    <tr>
	      <td>
		<table style="font-size:20px">
		  <tr><td>Previous morning</td><td>Previous day</td><td>Previous night</td><td>Early morning</td><td>Closed?</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Light</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Heavy</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Medium</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Heavy</td><td>Heavy</td><td>TRUE</td></tr>
		  <tr><td>Light</td><td>Heavy</td><td>Heavy</td><td>Medium</td><td>TRUE</td></tr>
		  <tr><td>Heavy</td><td>Medium</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Medium</td><td>Medium</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Medium</td><td>Light</td><td>FALSE</td></tr>
		  <tr><td>Light</td><td>Light</td><td>Light</td><td>Medium</td><td>TRUE</td></tr>
		  </table>
	      </td>
	      <td>
		<br>
		<p>Let's consider a linear regression:</p>
		<p style="font-size:24px">$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
		<p style="font-size:24px">$Closed = w_1 * Heavy + w_2 * Heavy$</p>
		<p style="font-size:24px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$+ w_3 * Light + w_4 * Light$</p>
		<p style="font-size:24px">$-0.02 = -0.19 * 2 + 0.07 * 2$</p>
		<p style="font-size:24px">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$+ 0.16 * 0 + 0.40 * 0$</p>
	      </td>
	    </tr>
	  </table>
	</section>

	<section>
	  <h2>ROC Curve</h2>
	  <p>When we did a regression we created an equation of the form:</p>
	  <p>$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
	  <p>$Closed$ is a number that we <b>convert</b> into True/False</p>
	</section>

	<section>
	  <h2>ROC Curve</h2>
	  <p>$Closed = w_1 * P_m + w_2 * P_d + w_3 * P_n + w_4 * E_m$</p>
	  <p>Example values of $Closed$ might actually be:</p>
	  <table>
	    <tr><td>Reality</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
	    <tr><td>Predictions</td><td>0.8</td><td>0.5</td><td>0.45</td><td>0.5</td><td>-0.35</td><td>-0.05</td></tr>
	  </table>
	</section>

	<section>
	  <h2>ROC Curve</h2>
	  <table>
	    <tr><td>Reality</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
	    <tr><td>Predictions</td><td>0.8</td><td>0.5</td><td>0.45</td><td>0.5</td><td>-0.35</td><td>-0.05</td></tr>
	  </table>
	  <img src="images/snowday_ROC.gif">
	 
	</section>			
	
	<section>
	  <h2>ROC Curves</h2>
	  <img src="http://scikit-learn.org/stable/_images/plot_roc_001.png" width=60%>
	  <p>An ROC curve taken over a larger test set.</p>
	</section>	
	</section>	

	<section>
	<section>
	  <h2>Different types of accuracy</h2>
	  <p>Sample error - Error measured on a subset of data</p>
	  <p>True error - Error measured on the underlying dataset (probably can't be measured)</p>
	</section>
	
	<section>
	  <h2>Sample error</h2>
	  <img src="images/snowdaytree_depth1.png">
	  <p>If we measured error by using the decision tree for each day this week, then we would have a sample error</p>
	</section>

	<section>
	  <h2>True error</h2>
	  <img src="images/snowdaytree_depth1.png">
	  <p>If there was no noise in deciding snow days <b>and</b> we had data for all permutations of attribute-values:</p>
	  <p style="font-size:28px">Previous morning, Previous day, Previous night, Early morning</p>
	</section>
	</section>

	<section>
	<section>
	  <h2>Validation Datasets</h2>
	  <ul>
	    <li>Partition the dataset into training and testing</li>
	    <li>Train model on training set</li>
	    <li>Measure performance on testing set</li>
	  </ul>
	  <p>If we repeat this $N$ times for some ML algorithm and average performance,</p>
	  <p>Then we might get a decent estimate of the performance</p>
	  <p>What could go wrong?</p>
	</section>

	<section>
	  <h2>Validation Datasets</h2>
	  <p>What could go wrong?</p>
	  <p>We might have correlation between our train/test sets across runs</p>
	</section>	

	<section>
	  <h2>Cross-Validation</h2>
	  <ul>
	    <li>Divide data into $k$ subsets (called folds)</li>
	    <li>For each k:
	      <ul>
		<li>Train on all folds except $k$</li>
		<li>Test on $k$</li>
	    </ul></li>
	    <li>Report average performance over all $k$</li>
	  </ul>
	  <p>Test sets do not overlap!</p>
	</section>

	<section>
	  <h2>Cross-Validation</h2>
	  <p>Consider a dataset where $N=100$, split into 4 folds</p>
	  <img src="images/class_proportion_4fold.png">
	</section>	
	
	<section>
	  <h2>Stratified Cross-Validation</h2>
	  <ul>
	    <li>Split classes into sets</li>
	    <li>Partition class sets into $k$ folds</li>
	    <li>Join folds from each class</li>
	  </ul>
	</section>

	<section>
	  <h2>Leave One Out</h2>
	  <p>k-fold cross-validation, where $k=N$</p>
	  <ul>
	    <li>Effective</li>
	    <li>High variance for each fold test</li>
	    <li>Expensive</li>
	  </ul>
	</section>

	<section>
	  <h2>Cross-validation</h2>
	  <p>Wait, why did we talk about this?</p>
	</section>

	<section>
	  <h2>Cross-validation</h2>
	  <p>Cross-validation gives a more reliable estimate of algorithm performance</p>
	  <p>We can use this to compare ML algorithms!</p>
	</section>		
	</section>

  <section>
-->

	<!--
    <section data-markdown>
      ## Reinforcement Learning

      ![Cover of Reinforcement Learning by Sutton and Barto](http://incompleteideas.net/sutton/book/cover.gif)

      [Reinforcement Learning: An Introduction by Sutton and Barto](http://incompleteideas.net/sutton/book/the-book.html)

    </section>

    <section data-markdown>
      ## Reinforcement Learning

      Learn actions to take using a reward signal to reinforce desired behaviors

      Uses:
      - Game playing (i.e. chess, backgammon, go)
      - Control (i.e. regulating components at a factory)
      - Navigation (i.e robot vacuum cleaner)


    </section>

  </section>

  <section>
    <section data-markdown>
      ## Agent-Environment

      ![Agent environment](http://kephale.github.io/TuftsCOMP135_Spring2016/Lecture13/images/suttonBarto_agentEnvironment.png)

      Agent transitions through states by making actions, while receiving rewards

    </section>


    <section data-markdown>
      ## N-Armed Bandit

      ![Multi-armed bandit cartoon](http://research.microsoft.com/en-us/projects/bandits/MAB-2.jpg)

      Action, $a$: pull one machine's arm

      Reward, $r$: payoff for that particular machine

      *Given some observations of $a$ and $r$ pairs, what is the best action to take?*

    </section>

    <section>
      <h2>Action-Value</h2>

      <p>The estimated value of an action after the $t$th observed reward is</p>

      <p>$Q_t(a) = \frac{r_1+r_2+...+r_{k_a}}{k_a}$</p>

      <p>where $k_a$ is the number of times that $a$ has been chosen. The true action value function will be denoted $Q^*(a)$</p>

    </section>

    <section data-markdown>
      ## Policies

      An agent selects an action, $a$, from a state, $s$, with a policy, $\pi$.

      **Example policies:**
      - *greedy*, always take the expected best action
      - *$\epsilon$-greedy*, greedy with $\epsilon$ probability of a random action
      - *softmax*, use a Boltzmann distribution

      A key characteristic of policies is the exploration-exploitation tradeoff.

    </section>


    <section data-markdown>
      ## Reinforcement Learning Variables

      There are a number of variables we'll be using:
      - $a$, action
      - $s$, state
      - $t$, timestep
      - $r$, reward
      - $\alpha$, learning rate
      - $\gamma \in [0,1)$, discounting rate
      - $V(s)$, state value function
      - $Q(s,a)$, state-action value function
      - $\pi(s)$, policy (maps to an action/probability)

    </section>
  </section>

  <section>
    <section>
      <h2>Back to Action-Value</h2>

      <p>The estimated value of an action after the $t$th observed reward is</p>

      <p>$Q_t(a) = \frac{r_1+r_2+...+r_{k_a}}{k_a}$</p>

      <p>where $k_a$ is the number of times that $a$ has been chosen. The true action value function will be denoted $Q^*(a)$</p>

      <p>What happens after billions of observed rewards?</p>

    </section>

    <section>
      <h2>Incremental Action-Value</h2>

      <p>Never fear, we can calculate $Q_t(a)$ incrementally</p>

      <p>$Q_t(a) = \frac{r_1+r_2+...+r_{k_a}}{k_a}$</p>

      <p>For some Q-value</p>

      <p>$Q_{k+1} = \frac{1}{k+1} \displaystyle \sum_{i=1}^{k+1} r_i$</p>

      <p>...</p>

      <p>$Q_{k+1} = Q_k + \frac{1}{k+1} [ r_{k+1} - Q_k ]$</p>

      <p><small><a href="http://incompleteideas.net/sutton/book/ebook/node19.html">More reading</a></small></p>

    </section>

    <section>
      <h2>Incremental Updates</h2>

      <p>A general form of the incremental update rule</p>

      <p>$NewEstimate \leftarrow OldEstimate + StepSize [Target - OldEstimate]$</p>

    </section>

  </section>


<section>
  <section data-markdown>
    ## Markov Property

    In general, the current state and reward depend on the entire sequence of observations

    ![State-reward depending on all previous states](http://incompleteideas.net/sutton/book/ebook/numeqtmp13.png)

    A problem with the *Markov* property only depends on the previous state

    ![State-reward depending on immediately previous state only](http://incompleteideas.net/sutton/book/ebook/numeqtmp14.png)

  </section>


  <section data-markdown>
    ## Markov Property

    Consider the task of balancing a pole

    ![Pole balancing](http://incompleteideas.net/sutton/book/ebook/figtmp8.png)

  </section>


  <section data-markdown>
    ## Markov Decision Processes

    RL problems that satisfy the Markov property are called Makov decission processes (MDPs)

    If the action and state space are finite, then it is a finite MDP, which is defined by

    Transition probabilities

      ![Transition probabilities](http://incompleteideas.net/sutton/book/ebook/numeqtmp15.png)

      and reward function

      ![Expected reward](http://incompleteideas.net/sutton/book/ebook/numeqtmp16.png)

    </body>

  </section>


  <section data-markdown>
    ## Value Functions

    Given that a policy, $\pi$, maps from a state-action pair to a probability of taking an action $\pi(s,a)$, the state-value function *under* policy $\pi$ can be written as

    ![State-value function](http://incompleteideas.net/sutton/book/ebook/numeqtmp17.png)

    and the action-value function can be written as

    ![Action-value function](http://incompleteideas.net/sutton/book/ebook/numeqtmp18.png)

    where $\gamma \in [0,1)$ is the discounting factor

  </section>

  <section data-markdown>
    ## Optimal Value Functions

    Value functions represent the amount of reward aquired by a policy over the long term.

    We can express the optimal value function as the value function for the best policy

    $V^*(s) = max_{\pi} V^{\pi}(s)$


  </section>

  <section data-markdown>
    ## Bellman Equation

    Let's look at the Bellman equation to ensure that our formulations are self-consistent

    [Bellman optimality equation](http://incompleteideas.net/sutton/book/ebook/node35.html)

  </section>

</section>

<section>

  <section data-markdown>
    ## Iterative Policy Evaluation

    A policy must be evaluated to quantify behavior via its value function.

    ![Iterative policy evaluation](http://incompleteideas.net/sutton/book/ebook/pseudotmp0.png)

    Algorithm for calculating $V(s)$ for a polict $\pi$

  </section>

  <section data-markdown>
    ## Iterative Policy Evaluation

    Consider a 4x4 gridworld example

    ![Gridworld example](http://incompleteideas.net/sutton/book/ebook/imgtmp4.png)

    Actions are: left, right, up, and down

  </section>

  <section data-markdown>
    ## Iterative Policy Evaluation

    ![Gridworld example](http://incompleteideas.net/sutton/book/ebook/figtmp15.png)

  </section>

  <section data-markdown>
    ## Policy Iteration

    The purpose of computing the value function is so we can find better policies.

    We are looking for a new policy, $\pi'$, such that $V^{\pi'}(s) \geq V^{\pi}(s)$

    How can we achieve this?

  </section>

  <section data-markdown>
    ## Policy Iteration

    ![Policy iteration](http://incompleteideas.net/sutton/book/ebook/pseudotmp1.png)

  </section>

</section>

<section>
  <section >
    <h2>Temporal-Difference Learning</h2>

    <p>For an episodic or continuous task we use this formulation to update the value function</p>

    <p>We express the sequence of rewards as the "return"</p>

    <p>$R_t = r_t + \gamma * r_{t+1} + \gamma^2 * r_{t+2} ... + \gamma^T * r_{T}$</p>

    <p>where $T$ is the terminal timestep</p>

    <p>Using this, we can write the update to the value function as</p>

    <p><img src="http://incompleteideas.net/sutton/book/ebook/numeqtmp28.png"></p>

  </section>


  <section data-markdown>
    ## Temporal-Difference Learning

    We know that the value function for our next state encodes respective the expected return.

    This suggests that we can learn from the differences values of successive states:

    ![TD update](http://incompleteideas.net/sutton/book/ebook/numeqtmp29.png)

    This is the temporal-difference (TD) update rule.

    Similar behavior has been observed in dopamine neurons (Schultz, 1998)

  </section>


  <section data-markdown>
    ## Temporal-Difference Learning

    We can also write a TD update rule for $Q(s,a)$

    ![SARSA update](http://incompleteideas.net/sutton/book/ebook/numeqtmp30.png)

    Can we design an on-policy learning algorithm (i.e an algorithm that learns and uses what it has learned to make decisions)?

  </section>

  <section data-markdown>
    ## SARSA

    The creatively names SARSA (state-action-reward-state-action) algorithm is an on-policy TD algorithm

    ![SARSA code](http://incompleteideas.net/sutton/book/ebook/pseudotmp8.png)

    Off-policy alternative to this algorithm are called "Q-learning"

  </section>


</section>

<section>

  <section data-markdown>
    ## Eligibility Traces

    TD-learning only propagates information from the subsequent state, and may be slow.

    ![Backups](http://incompleteideas.net/sutton/book/ebook/figtmp36.png)

    How can we resolve this?

  </section>

  <section data-markdown>
    ## Eligibility Traces

    Eligibility traces are an additional variable associated with each state, encoding the time-since-last-visit as

    ![eligibility trace math](http://incompleteideas.net/sutton/book/ebook/numeqtmp36.png)

    which may be visualized as

    ![Eligibility plot](http://incompleteideas.net/sutton/book/ebook/imgtmp15.png)

  </section>

  <section data-markdown>
    ## Eligibility Traces

    We can adapt our learning algorithm to use these traces as

    ![TD lambda](http://incompleteideas.net/sutton/book/ebook/pseudotmp11.png)

  </section>

  <section data-markdown>
    ## Function Approximation

    What about RL in a situation like the game of Go?

    There are $3^{19*19}$ states of a Go board. Can we continue to use a lookup table for our functions?

    [AlphaGo Slides](http://kephale.github.io/TuftsCOMP135_Spring2016/Lecture13/#/5/6)
  </section>

</section>
-->
	
    <section>
      <h2>The Game of Go</h2>
      <img src="https://upload.wikimedia.org/wikipedia/commons/2/2a/FloorGoban.JPG" width=60%>
    </section>


  <section>
    <h2>The Game of Go</h2>
    <p>2 players, 19 by 19 board</p>
    <p>$10^{761}$ possible games (# chess games $\leq 40$ moves $\approx 10^{43}$)</p>
    <p>Goal: encircle opponent's pieces to claim territory</p>
    <img src="https://upload.wikimedia.org/wikipedia/commons/d/dc/Golibs.png" width=30%>
  </section>
  </section>

  <section>
  <section>
    <h2>AlphaGo</h2>
    <ul>
      <li>Made by DeepMind (now owned by Google)</li>
      <li>AI that uses deep convolutional neural nets to play Go</li>
      <li>Has already beat a professional player (Fan Hui)</li>
      <li>Has now won the first 2 of 5 matches against world champion Lee Sedol</li>
      <li>Published in <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Nature</a></li>
    </ul>
  </section>

  <section>
    <h2>AlphaGo</h2>
    <p>Uses convolutional neural networks for processing/representing the game</p>
    <p>Trained with expert data and self-play using reinforcement learning</p>
  </section>

  <section>
    <h2>AlphaGo</h2>
    <p>Day 2 of competition - <a href="https://youtu.be/l-GsfyVCBu0?t=1891">start</a></p>
  </section>


  <section>
    <h2>AlphaGo</h2>
    <ol>
      <li>Supervised learning of expert moves</li>
      <li>Train a fast player for use in tree search</li>
      <li>Use reinforcement learning to optimize beyond expert moves (instead of memorize them)</li>
    </ol>
  </section>
  </section>

  <section>
  <section>
    <h2>Machine Learning for Games</h2>
    <p>What are the machine learning questions in gameplay?</p>
  </section>

  <section>
    <h2>Machine Learning for Games</h2>
    <p>What are the machine learning questions in gameplay?</p>
    <ul>
      <li>Classifying/learning expert moves</li>
      <li>Ranking board configurations</li>
      <li>Learning moves to make</li>
      <li>...</li>
    </ul>
  </section>

  <section>
    <h2>Terminology for Agents</h2>
    <p>Game AI and reinforcement learning use an agent-centric terminology</p>
    <ul>
      <li>state, $s$: state of environment/game board</li>
      <li>action, $a$: agents perform actions which generally lead to changes in state</li>
      <li>policy, $\pi$: defines an agent's behavior, roughly speaking a mapping from states to actions</li>
    </ul>
  </section>

  <section>
    <h2>Minimax Tree Search</h2>
    <p>In 2 player, zero-sum games both players want to win</p>
    <p>From some state of the game, we can predict a sequence of alternating actions where</p>
    <ul>
      <li>Self: maximizes the next state of the board</li>
      <li>Opponent: minimizes (with respect to self) the next state of the board</li>
    </ul>
  </section>

  <section>
    <h2>Minimax Tree Search</h2>
    <img src="https://upload.wikimedia.org/wikipedia/commons/e/e1/Plminmax.gif" width=60%>
    <p><small>Image from Maschelos at English Wikipedia</small></p>
  </section>

  <section>
    <h2>Monte Carlo Tree Search</h2>
    <p>MCTS is a randomized algorithm to sample possible outcomes</p>
    <p>Idea: Simulate game play from a relevant board state and store outcome. Do this many times.</p>
  </section>

  <section>
    <h2>Monte Carlo Tree Search</h2>
    <ol>
      <li>Selection: grow tree in a promising direction to node L</li><br>
      <li>Expansion: If L is not terminal, choose a child node C</li><br>
      <li>Simulation: "randomly" play the game starting from C</li><br>
      <li>Backpropagation: store the result of simulation in the nodes starting at C</li>
    </ol>
  </section>

  <section>
    <h2>Monte Carlo Tree Search</h2>
    <p>But what if the game tree is huge?</p>
    <p>There are about $10^{761}$ possible paths in the Go game tree</p>
  </section>
  </section>

  <section>

    <section>
      <h2>Go and Neural Nets</h2>
      <p>A Go board is basically an image (2D pixels with value: empty/black/white)</p>
      <img src="https://upload.wikimedia.org/wikipedia/commons/a/aa/Gokof.png" width=40%>
    </section>


  <section>
    <h2>Convolutional Neural Networks</h2>
    <p>Input neurons in CNNs have "receptive fields" that cover patches of an input</p>
    <img src="http://ufldl.stanford.edu/tutorial/images/Cnn_layer.png" width=40%>
    <p><small>Image from <a href="http://ufldl.stanford.edu/tutorial/">UFLDL, Stanford</a></small></p>
  </section>

  <section>
    <h2>Convolution</h2>
    <p>Convolution is the process of taking a kernel, sliding it over an input image, and taking an innner product</p>
    <img src="http://ufldl.stanford.edu/tutorial/images/Convolution_schematic.gif">
    <p><small>Image from <a href="http://ufldl.stanford.edu/tutorial/">UFLDL, Stanford</a></small></p>
  </section>

  <section>
    <h2>Pooling</h2>
    <p>Pooling is an aggregation over a pool of units/neurons<p>
    <img src="http://ufldl.stanford.edu/tutorial/images/Pooling_schematic.gif" width=70%>
    <p><small>Image from <a href="http://ufldl.stanford.edu/tutorial/">UFLDL, Stanford</a></small></p>
  </section>

  <section>
    <h2>Softmax Function</h2>
    <p>Generalized logistic function, to squash K-dimensional values</p>
    <p>Choose action $a$ with probability:</p>
    <p>$\frac{e^{Q_t(a) / \tau}}{\sum^K_{b=1} e^{Q_t(b)/ \tau}}$</p>
    <p>where $Q_t(a)$ is the value of action $a$ at time $t$</p>
  </section>

  <section>
    <h2>Training CNNs</h2>
    <p>Normal neural network training methods apply</p>
    <ul>
      <li>Backpropagation</li>
      <li>Stochastic gradient descent</li>
    </ul>
  </section>

  <section>
    <h2>AlphaGo's Network Structure</h2>
    <img src="images/alphaGo_NetworkStructure.png" width=50%>
    <p><small>Image from Silver et al, 2016</small></p>
  </section>
  </section>

  <section>
  <section>
    <h2>Reinforcement Learning</h2>
    <p>Agent transitions through states by making actions, while receiving rewards</p>
    <img src="images/suttonBarto_agentEnvironment.png">
    <p><small>Image from the RL <a href="http://incompleteideas.net/sutton/book/the-book.html">book</a> by Sutton and Barto</small></p>
  </section>

  <section>
    <h2>Reinforcement Learning</h2>
    <p>In RL, agents attempt to maximize reward obtained in the long-term</p>
    <p>Rewards can be described as a summed sequence:</p>
    <p>$R_t = r_{t} + r_{t+1} + r_{t+2} + ... + r_{T}$</p>
  </section>

  <section>
    <h2>Reinforcement Learning</h2>
    <p>The core of most RL algorithms is to estimate a value function:</p>
    <p>$V^{\pi} (s) = E_{\pi} \{ R_t | s_t = s \}$</p>
  </section>

  <section>
    <h2>Reinforcement Learning</h2>
    <p>Learning the value function $V^{\pi}$ is accomplished by trial-and-error and reinforcement via reward signal</p>
    <p>Dynamic programming is used to do this</p>
    <p>We'll cover specifics in the RL lectures</p>
  </section>
  </section>

	
      </div>
      
    </div>
    
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    
    <script>
      
      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
      controls: true,
      progress: true,
      history: true,
      center: true,
      
      transition: 'slide', // none/fade/slide/convex/concave/zoom
      
      // Optional reveal.js plugins
      dependencies: [
      { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'plugin/zoom-js/zoom.js', async: true },
      { src: 'plugin/notes/notes.js', async: true },
      { src: 'plugin/math/math.js', async: true }                 
      ]
      });
      
    </script>
    
  </body>
</html>

